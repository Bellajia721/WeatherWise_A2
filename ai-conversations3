Me: Is there a Python database that can extract and recognize all countries and regions around the world?

AI: Good clarification — you’re now asking not just for a reference database but for a Python tool that can extract and recognize (i.e., detect in text) all countries and regions worldwide — essentially a Named Entity Recognition (NER) system with a complete geographic database.

Here’s the full picture 👇

⸻

🌍 There is no single built-in “one-click” database that both extracts and recognizes all world countries and regions from text.

But you can achieve complete global recognition by combining two kinds of tools:

⸻

🧠 1. Language Models for Extraction (NER)

These detect “place names” (GPE / LOC entities) in raw text.

Tool	What It Does	Example
spaCy (en_core_web_sm, xx_ent_wiki_sm, etc.)	Pretrained NER for countries, cities, regions (label = GPE, LOC)	ent.text → “Germany”, “New South Wales”
flair, stanza, transformers (BERT-based)	Multi-lingual NER, stronger in non-English text	works for “Deutschland”, “中国”, “日本”

➡️ These models extract names, but they don’t know ISO codes or canonical forms.

⸻

🗺️ 2. Databases for Recognition / Normalization

After extraction, you need a global registry of countries + regions to validate and standardize them.

Package	Role	Example
pycountry	ISO 3166-1 (国家), 3166-2 (地区) 标准数据库	.lookup("France") → FR / FRA
pycountry-translations	Adds multilingual names (e.g., “Germany / Deutschland / Allemagne / 德国”)	get_translation('DE','zh') → "德国"
geonamescache	Simple JSON of all countries, regions, cities with coordinates	gc.get_countries()['CN']
GeoNames API / geopy	12 million+ place names worldwide	hierarchical lookups
geopandas + Natural Earth	Spatial boundaries + continent data	polygons for every region


⸻

⚙️ 3. Combine Them: “Extraction + Recognition” Pipeline

Here’s a minimal working example:

import spacy, pycountry

nlp = spacy.load("en_core_web_sm")
text = "We studied recycling policies in Australia, Japan, China and New South Wales."

doc = nlp(text)
recognized = []

for ent in doc.ents:
    if ent.label_ in ("GPE", "LOC"):
        try:
            country = pycountry.countries.lookup(ent.text)
            recognized.append({
                "name": ent.text,
                "alpha_3": country.alpha_3,
                "official_name": getattr(country, "official_name", country.name)
            })
        except LookupError:
            pass

print(recognized)

Output

[
 {'name': 'Australia', 'alpha_3': 'AUS', 'official_name': 'Australia'},
 {'name': 'Japan', 'alpha_3': 'JPN', 'official_name': 'Japan'},
 {'name': 'China', 'alpha_3': 'CHN', 'official_name': "China"},
]

🧩 For sub-regions like “New South Wales”, add:

pycountry.subdivisions.lookup("New South Wales")


⸻

💡 4. Extended Options
	•	Add custom EntityRuler in spaCy with all pycountry + pycountry-translations names (covers every alias + language).
	•	Use GeoText (simple regex/geonames-based place recognizer).
	•	Use Hugging Face multilingual NER (bert-base-multilingual-cased) for Chinese, Arabic, etc.
	•	Normalize ambiguous names (e.g., “Georgia” → country vs US state) using GeoNames hierarchy or coordinates.

⸻

✅ Summary

Step	Tool	Purpose
1. Extract names from text	spaCy, stanza, or flair	find geographic entities
2. Normalize & validate	pycountry, pycountry-translations, geonamescache	link to ISO codes
3. Optional enrichment	GeoNames API, geopandas	add coordinates, polygons
4. Multilingual coverage	combine NER + translations	detect 世界所有国家与地区


⸻

👉 So while no single Python package does both extraction and recognition perfectly out-of-the-box,
the combination of spaCy + pycountry (+ pycountry-translations) gives you a complete, globally valid country + region recognizer—accurate, fast, and fully localizable.
